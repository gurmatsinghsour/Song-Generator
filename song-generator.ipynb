{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:15.306342Z",
     "iopub.status.busy": "2025-07-13T13:42:15.305740Z",
     "iopub.status.idle": "2025-07-13T13:42:16.051017Z",
     "shell.execute_reply": "2025-07-13T13:42:16.050306Z",
     "shell.execute_reply.started": "2025-07-13T13:42:15.306316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:16.052427Z",
     "iopub.status.busy": "2025-07-13T13:42:16.052156Z",
     "iopub.status.idle": "2025-07-13T13:42:24.298783Z",
     "shell.execute_reply": "2025-07-13T13:42:24.297995Z",
     "shell.execute_reply.started": "2025-07-13T13:42:16.052409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:24.300010Z",
     "iopub.status.busy": "2025-07-13T13:42:24.299524Z",
     "iopub.status.idle": "2025-07-13T13:42:28.132535Z",
     "shell.execute_reply": "2025-07-13T13:42:28.131610Z",
     "shell.execute_reply.started": "2025-07-13T13:42:24.299988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/gurmatsinghsour/miniconda3/envs/ml-gpu/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/gurmatsinghsour/miniconda3/envs/ml-gpu/lib/python3.11/site-packages (from langdetect) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:28.134748Z",
     "iopub.status.busy": "2025-07-13T13:42:28.133896Z",
     "iopub.status.idle": "2025-07-13T13:42:32.373268Z",
     "shell.execute_reply": "2025-07-13T13:42:32.372411Z",
     "shell.execute_reply.started": "2025-07-13T13:42:28.134712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('song_lyrics.csv', usecols=['lyrics'], nrows=50)\n",
    "\n",
    "import langdetect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return langdetect.detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df = df[df['lyrics'].apply(is_english)]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:32.375780Z",
     "iopub.status.busy": "2025-07-13T13:42:32.375516Z",
     "iopub.status.idle": "2025-07-13T13:42:32.824367Z",
     "shell.execute_reply": "2025-07-13T13:42:32.823762Z",
     "shell.execute_reply.started": "2025-07-13T13:42:32.375758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[,\\.!?()]', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\n\\']', '', text)\n",
    "    return text\n",
    "\n",
    "df['lyrics'] = df['lyrics'].apply(clean_text)\n",
    "df = df[df['lyrics'].str.strip().astype(bool)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:32.825433Z",
     "iopub.status.busy": "2025-07-13T13:42:32.825167Z",
     "iopub.status.idle": "2025-07-13T13:42:43.705009Z",
     "shell.execute_reply": "2025-07-13T13:42:43.704284Z",
     "shell.execute_reply.started": "2025-07-13T13:42:32.825408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['lyrics'])\n",
    "\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['lyrics'])\n",
    "\n",
    "sequences = [seq for seq in sequences if len(seq) > 5]\n",
    "\n",
    "input_sequences = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_seq = seq[:i+1]\n",
    "        input_sequences.append(n_gram_seq)\n",
    "\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:43.706073Z",
     "iopub.status.busy": "2025-07-13T13:42:43.705842Z",
     "iopub.status.idle": "2025-07-13T13:42:43.993415Z",
     "shell.execute_reply": "2025-07-13T13:42:43.992621Z",
     "shell.execute_reply.started": "2025-07-13T13:42:43.706052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurmatsinghsour/miniconda3/envs/ml-gpu/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_seq_len - 1))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:43.994465Z",
     "iopub.status.busy": "2025-07-13T13:42:43.994205Z",
     "iopub.status.idle": "2025-07-13T13:42:46.476029Z",
     "shell.execute_reply": "2025-07-13T13:42:46.475156Z",
     "shell.execute_reply.started": "2025-07-13T13:42:43.994433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "input_sequences = np.array(input_sequences)\n",
    "X = input_sequences[:, :-1]\n",
    "y = to_categorical(input_sequences[:, -1], num_classes=len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:46.477269Z",
     "iopub.status.busy": "2025-07-13T13:42:46.477021Z",
     "iopub.status.idle": "2025-07-13T13:42:46.492904Z",
     "shell.execute_reply": "2025-07-13T13:42:46.492286Z",
     "shell.execute_reply.started": "2025-07-13T13:42:46.477249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_seq_len - 1))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:42:46.493787Z",
     "iopub.status.busy": "2025-07-13T13:42:46.493563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 154ms/step - accuracy: 0.0466 - loss: 6.5917\n",
      "Epoch 2/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - accuracy: 0.0479 - loss: 5.9419\n",
      "Epoch 3/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - accuracy: 0.0572 - loss: 5.7470\n",
      "Epoch 4/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 154ms/step - accuracy: 0.0646 - loss: 5.5334\n",
      "Epoch 5/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 154ms/step - accuracy: 0.0739 - loss: 5.4024\n",
      "Epoch 6/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 155ms/step - accuracy: 0.0893 - loss: 5.2436\n",
      "Epoch 7/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 154ms/step - accuracy: 0.1057 - loss: 5.0444\n",
      "Epoch 8/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - accuracy: 0.1305 - loss: 4.8287\n",
      "Epoch 9/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - accuracy: 0.1599 - loss: 4.5956\n",
      "Epoch 10/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - accuracy: 0.1813 - loss: 4.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=64)\n",
    "model.save(\"song_generator_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the way you i be what you got me i i be you\n"
     ]
    }
   ],
   "source": [
    "def generate_song_line(seed_text, next_words=50):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        output_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        if output_word is None:\n",
    "            break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "generate_song_line = generate_song_line(\"I love the way you\", next_words=10)\n",
    "print(generate_song_line)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2805070,
     "sourceId": 4840139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
